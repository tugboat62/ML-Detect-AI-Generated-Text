{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6847931,"sourceType":"datasetVersion","datasetId":3936750},{"sourceId":6867914,"sourceType":"datasetVersion","datasetId":3946973},{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7594817,"sourceType":"datasetVersion","datasetId":4420659},{"sourceId":7689680,"sourceType":"datasetVersion","datasetId":4481355},{"sourceId":7692097,"sourceType":"datasetVersion","datasetId":4487481},{"sourceId":7680411,"sourceType":"datasetVersion","datasetId":4480872},{"sourceId":7680078,"sourceType":"datasetVersion","datasetId":4480672},{"sourceId":11673,"sourceType":"modelInstanceVersion","modelInstanceId":9424}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-24T14:17:57.019229Z","iopub.execute_input":"2024-02-24T14:17:57.01983Z","iopub.status.idle":"2024-02-24T14:17:57.046647Z","shell.execute_reply.started":"2024-02-24T14:17:57.019782Z","shell.execute_reply":"2024-02-24T14:17:57.045747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:17:57.048573Z","iopub.execute_input":"2024-02-24T14:17:57.049197Z","iopub.status.idle":"2024-02-24T14:17:57.053852Z","shell.execute_reply.started":"2024-02-24T14:17:57.049163Z","shell.execute_reply":"2024-02-24T14:17:57.052747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:17:57.054991Z","iopub.execute_input":"2024-02-24T14:17:57.055338Z","iopub.status.idle":"2024-02-24T14:17:57.065471Z","shell.execute_reply.started":"2024-02-24T14:17:57.055303Z","shell.execute_reply":"2024-02-24T14:17:57.064651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"date_set = pd.read_csv(\"/kaggle/input/ai-generated-text-preprocessed/preprocessed_data.csv\")\ndate_set.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:17:57.067779Z","iopub.execute_input":"2024-02-24T14:17:57.068133Z","iopub.status.idle":"2024-02-24T14:18:02.284158Z","shell.execute_reply.started":"2024-02-24T14:17:57.068092Z","shell.execute_reply":"2024-02-24T14:18:02.28322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_essays_final_shuffled = date_set.sample(frac=1).reset_index(drop=True)\ndf_train_essays_final = df_train_essays_final_shuffled[100000:180000]","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:02.285224Z","iopub.execute_input":"2024-02-24T14:18:02.285497Z","iopub.status.idle":"2024-02-24T14:18:02.329164Z","shell.execute_reply.started":"2024-02-24T14:18:02.285475Z","shell.execute_reply":"2024-02-24T14:18:02.328406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_essays_final['generated'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:02.330208Z","iopub.execute_input":"2024-02-24T14:18:02.330499Z","iopub.status.idle":"2024-02-24T14:18:02.337402Z","shell.execute_reply.started":"2024-02-24T14:18:02.330476Z","shell.execute_reply":"2024-02-24T14:18:02.336466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"only_zero_rows = df_train_essays_final[df_train_essays_final['generated'] == 0]\nonly_one_rows = df_train_essays_final[df_train_essays_final['generated'] == 1]","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:02.338605Z","iopub.execute_input":"2024-02-24T14:18:02.338882Z","iopub.status.idle":"2024-02-24T14:18:02.349279Z","shell.execute_reply.started":"2024-02-24T14:18:02.338858Z","shell.execute_reply":"2024-02-24T14:18:02.34819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"only_one_rows.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:02.352165Z","iopub.execute_input":"2024-02-24T14:18:02.352674Z","iopub.status.idle":"2024-02-24T14:18:02.360245Z","shell.execute_reply.started":"2024-02-24T14:18:02.352649Z","shell.execute_reply":"2024-02-24T14:18:02.359411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = min(only_one_rows.shape[0],only_zero_rows.shape[0])\nprint(rows)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:02.361298Z","iopub.execute_input":"2024-02-24T14:18:02.361631Z","iopub.status.idle":"2024-02-24T14:18:02.370979Z","shell.execute_reply.started":"2024-02-24T14:18:02.361602Z","shell.execute_reply":"2024-02-24T14:18:02.370119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"equal_one_rows = only_one_rows[:rows]\nequal_zero_rows = only_zero_rows[:rows]\n\n# Create a combined dataset with exactly 10 1s and 10 0s\ncombined_dataset = pd.concat([equal_one_rows, equal_zero_rows])\ncombined_dataset['generated'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:02.371929Z","iopub.execute_input":"2024-02-24T14:18:02.372166Z","iopub.status.idle":"2024-02-24T14:18:02.386877Z","shell.execute_reply.started":"2024-02-24T14:18:02.372145Z","shell.execute_reply":"2024-02-24T14:18:02.386138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_dataset['generated'].value_counts().plot(kind='bar', rot=0, color=['blue', 'orange'])\n\nplt.xlabel('Values')\nplt.ylabel('Count')\nplt.title('Bar Plot of 1s and 0s')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:02.387963Z","iopub.execute_input":"2024-02-24T14:18:02.388229Z","iopub.status.idle":"2024-02-24T14:18:02.585593Z","shell.execute_reply.started":"2024-02-24T14:18:02.388207Z","shell.execute_reply":"2024-02-24T14:18:02.584689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(combined_dataset['essay'],combined_dataset['generated'])","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:02.586785Z","iopub.execute_input":"2024-02-24T14:18:02.587144Z","iopub.status.idle":"2024-02-24T14:18:02.593845Z","shell.execute_reply.started":"2024-02-24T14:18:02.58711Z","shell.execute_reply":"2024-02-24T14:18:02.592775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling bert with distilbert ","metadata":{}},{"cell_type":"markdown","source":"### Load the saved bert model ","metadata":{}},{"cell_type":"code","source":"bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\nbert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:02.595205Z","iopub.execute_input":"2024-02-24T14:18:02.595649Z","iopub.status.idle":"2024-02-24T14:18:21.760165Z","shell.execute_reply.started":"2024-02-24T14:18:02.595577Z","shell.execute_reply":"2024-02-24T14:18:21.759175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall')\n]\n# Create a new model with the same architecture\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers\nl = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\nl = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n\nnew_model = tf.keras.Model(inputs=[text_input], outputs=[l])\nnew_model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=METRICS)\n\n# Load the saved weights into the new model\nnew_model.load_weights('/kaggle/input/model-weights/my_model_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:21.761426Z","iopub.execute_input":"2024-02-24T14:18:21.761715Z","iopub.status.idle":"2024-02-24T14:18:28.368303Z","shell.execute_reply.started":"2024-02-24T14:18:21.761689Z","shell.execute_reply":"2024-02-24T14:18:28.367425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the saved distilbert model ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import activations, optimizers, losses\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n\nMODEL_NAME = 'distilbert-base-uncased'\nN_EPOCHS = 10\n\nmodel_distilbert = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\noptimizer = optimizers.Adam(learning_rate=3e-5)\nloss_fn  = losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel_distilbert.compile(optimizer=optimizer, loss=loss_fn , metrics=['accuracy'])\nmodel_distilbert.load_weights('/kaggle/input/distilbert-ml-project/new_distilbert_model_weights.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:28.369643Z","iopub.execute_input":"2024-02-24T14:18:28.369915Z","iopub.status.idle":"2024-02-24T14:18:38.713004Z","shell.execute_reply.started":"2024-02-24T14:18:28.369891Z","shell.execute_reply":"2024-02-24T14:18:38.711994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare data for distilbert model","metadata":{}},{"cell_type":"code","source":"def construct_encodings(x, tkzr, trucation=True, padding=True):\n    return tkzr(x, truncation=trucation, padding=padding)\n\ndef construct_tfdataset(encodings, y=None):\n    if y:\n        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n    else:\n        # this case is used when making predictions on unseen samples after training\n        return tf.data.Dataset.from_tensor_slices(dict(encodings))  \n\ntkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n\n## it will create X,y\ndef create_tf_set(dataX, dataY, tkzr):\n    encodings = construct_encodings(dataX, tkzr)\n    tfdataset = construct_tfdataset(encodings, dataY)\n    return tfdataset    \n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:38.714305Z","iopub.execute_input":"2024-02-24T14:18:38.715112Z","iopub.status.idle":"2024-02-24T14:18:39.380988Z","shell.execute_reply.started":"2024-02-24T14:18:38.715075Z","shell.execute_reply":"2024-02-24T14:18:39.380099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_list = X_test.tolist()\ny_test_list = y_test.tolist()\nX_train_list = X_train.tolist()\ny_train_list = y_train.tolist()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:39.382545Z","iopub.execute_input":"2024-02-24T14:18:39.38286Z","iopub.status.idle":"2024-02-24T14:18:39.38754Z","shell.execute_reply.started":"2024-02-24T14:18:39.382833Z","shell.execute_reply":"2024-02-24T14:18:39.386432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_test = create_tf_set(X_test_list,y_test_list,tkzr)\ntf_train = create_tf_set(X_train_list,y_train_list,tkzr)\nBATCH_SIZE = 16\ntfdataset_test = tf_test.batch(BATCH_SIZE)\ntfdataset_train = tf_train.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:39.389044Z","iopub.execute_input":"2024-02-24T14:18:39.38935Z","iopub.status.idle":"2024-02-24T14:18:39.444751Z","shell.execute_reply.started":"2024-02-24T14:18:39.389324Z","shell.execute_reply":"2024-02-24T14:18:39.443944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val = model_distilbert.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\nprint(val)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:39.44846Z","iopub.execute_input":"2024-02-24T14:18:39.448817Z","iopub.status.idle":"2024-02-24T14:18:46.088646Z","shell.execute_reply.started":"2024-02-24T14:18:39.448793Z","shell.execute_reply":"2024-02-24T14:18:46.087617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling averaging two models' predictions ","metadata":{}},{"cell_type":"code","source":"preds_model1 = new_model.predict(X_test)\n# preds_model2 = loaded_xlnet.predict(test_input_ids)\npreds_model2 = model_distilbert.predict(tfdataset_test)\n# print(preds_model1.shape)\npreds_model2 = preds_model2.logits\n\n# Convert logits to probabilities using softmax\npreds_model2 = tf.nn.softmax(preds_model2, axis=-1)\npreds_model2 = preds_model2[:, 1].numpy()\npreds_model2 = preds_model2.reshape(-1, 1)\n# print(preds_model2.shape)\nensemble_preds = (preds_model1 + preds_model2) / 2\n# print(ensemble_preds[0])","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:46.089727Z","iopub.execute_input":"2024-02-24T14:18:46.090024Z","iopub.status.idle":"2024-02-24T14:18:48.552421Z","shell.execute_reply.started":"2024-02-24T14:18:46.089997Z","shell.execute_reply":"2024-02-24T14:18:48.551582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef calc_accuracy(ensemble_preds, y_test):\n    \n    y_pred = ensemble_preds.flatten()\n#     print(y_pred.shape, ensemble_preds.shape)\n    y_pred = np.where(y_pred > 0.5, 1, 0)\n    # Assuming y_pred and y_test are flattened arrays with 0/1 values\n    accuracy = accuracy_score(y_test, y_pred)\n    conf_matrix = confusion_matrix(y_test, y_pred)\n\n    print(f'Accuracy: {accuracy * 100:.2f}%')\n    \n    # Plot confusion matrix using seaborn\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(2), yticklabels=range(2))\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:48.553476Z","iopub.execute_input":"2024-02-24T14:18:48.553747Z","iopub.status.idle":"2024-02-24T14:18:48.657327Z","shell.execute_reply.started":"2024-02-24T14:18:48.553724Z","shell.execute_reply":"2024-02-24T14:18:48.656567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(y_test), len(ensemble_preds))\ncalc_accuracy(ensemble_preds, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:48.658437Z","iopub.execute_input":"2024-02-24T14:18:48.658726Z","iopub.status.idle":"2024-02-24T14:18:48.993889Z","shell.execute_reply.started":"2024-02-24T14:18:48.658702Z","shell.execute_reply":"2024-02-24T14:18:48.992971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling using voting ","metadata":{}},{"cell_type":"code","source":"preds_model1 = (new_model.predict(X_test) > 0.5).astype(int)\n# preds_model2 = (loaded_xlnet.predict(test_input_ids) > 0.5).astype(int)\npreds_model2 = model_distilbert.predict(tfdataset_test)\n# print(preds_model1.shape)\npreds_model2 = preds_model2.logits\n\n# Convert logits to probabilities using softmax\npreds_model2 = tf.nn.softmax(preds_model2, axis=-1)\npreds_model2 = preds_model2[:, 1].numpy()\npreds_model2 = preds_model2.reshape(-1, 1)\npreds_model2 = (preds_model2 > 0.5).astype(int)\nensemble_preds = (preds_model1 + preds_model2) >= 1","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:48.995164Z","iopub.execute_input":"2024-02-24T14:18:48.995493Z","iopub.status.idle":"2024-02-24T14:18:49.129597Z","shell.execute_reply.started":"2024-02-24T14:18:48.995467Z","shell.execute_reply":"2024-02-24T14:18:49.12884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calc_accuracy(ensemble_preds, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:49.130702Z","iopub.execute_input":"2024-02-24T14:18:49.131006Z","iopub.status.idle":"2024-02-24T14:18:49.392063Z","shell.execute_reply.started":"2024-02-24T14:18:49.13098Z","shell.execute_reply":"2024-02-24T14:18:49.391213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling using stacking","metadata":{}},{"cell_type":"code","source":"# preds_model1_train = new_model.predict(X_train)\n# preds_model2_train = loaded_xlnet.predict(train_input_ids)\n\n# meta_model_input = np.concatenate([preds_model1_train, preds_model2_train], axis=1)\n\n# meta_model = tf.keras.Sequential([\n#     tf.keras.layers.Dense(1, activation='sigmoid', input_dim=2),  # Adjust input_dim based on the number of base models\n# ])\n\n# meta_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# meta_model.fit(meta_model_input, y_train, epochs=10, batch_size=32)\n\n# # Make predictions on test data\n# preds_model1_test = new_model.predict(X_test)\n# preds_model2_test = loaded_xlnet.predict(test_input_ids)\n\n# meta_model_input_test = np.concatenate([preds_model1_test, preds_model2_test], axis=1)\n\n# ensemble_preds = meta_model.predict(meta_model_input_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:49.393079Z","iopub.execute_input":"2024-02-24T14:18:49.393342Z","iopub.status.idle":"2024-02-24T14:18:49.398117Z","shell.execute_reply.started":"2024-02-24T14:18:49.393318Z","shell.execute_reply":"2024-02-24T14:18:49.397212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_model1_train = new_model.predict(X_train)\npreds_model2_train = model_distilbert.predict(tfdataset_train)\npreds_model2_train = preds_model2_train.logits\n\n# Convert logits to probabilities using softmax\npreds_model2_train = tf.nn.softmax(preds_model2_train, axis=-1)\npreds_model2_train = preds_model2_train[:, 1].numpy()\npreds_model2_train = preds_model2_train.reshape(-1, 1)\n\n\nmeta_model_input = np.concatenate([preds_model1_train, preds_model2_train], axis=1)\n\nmeta_model = tf.keras.Sequential([\n    tf.keras.layers.Dense(1, activation='sigmoid', input_dim=2),  # Adjust input_dim based on the number of base models\n])\n\nmeta_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmeta_model.fit(meta_model_input, y_train, epochs=10, batch_size=32)\n\n# Make predictions on test data\npreds_model1_test = new_model.predict(X_test)\npreds_model2_test = model_distilbert.predict(tfdataset_test)\n# print(preds_model1.shape)\npreds_model2_test = preds_model2_test.logits\n\n# Convert logits to probabilities using softmax\npreds_model2_test = tf.nn.softmax(preds_model2_test, axis=-1)\npreds_model2_test = preds_model2_test[:, 1].numpy()\npreds_model2_test = preds_model2_test.reshape(-1, 1)\n\nmeta_model_input_test = np.concatenate([preds_model1_test, preds_model2_test], axis=1)\n\nensemble_preds = meta_model.predict(meta_model_input_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:49.399268Z","iopub.execute_input":"2024-02-24T14:18:49.399573Z","iopub.status.idle":"2024-02-24T14:18:52.929313Z","shell.execute_reply.started":"2024-02-24T14:18:49.399548Z","shell.execute_reply":"2024-02-24T14:18:52.928338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calc_accuracy(ensemble_preds, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:18:52.93069Z","iopub.execute_input":"2024-02-24T14:18:52.930988Z","iopub.status.idle":"2024-02-24T14:18:53.219732Z","shell.execute_reply.started":"2024-02-24T14:18:52.930961Z","shell.execute_reply":"2024-02-24T14:18:53.218805Z"},"trusted":true},"execution_count":null,"outputs":[]}]}